{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gMdpHPcQ6-H"
      },
      "source": [
        "### Implementing RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdhiedtzQyXH",
        "outputId": "8d0e8160-573d-4dfd-c904-6b7551948dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default data type: torch.float64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from scipy.integrate import solve_ivp\n",
        "#!pip install tikzplotlib #uncomment for saving nice images\n",
        "#import tikzplotlib\n",
        "# set precision\n",
        "torch.set_default_dtype(torch.float64)\n",
        "print('Default data type:', torch.get_default_dtype())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcHBJxunSiYs"
      },
      "source": [
        "Constructing ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbElWT5cR88B"
      },
      "outputs": [],
      "source": [
        "def antiderivTanh(x): # activation function aka the antiderivative of tanh\n",
        "    return torch.abs(x) + torch.log(1+torch.exp(-2.0*torch.abs(x)))\n",
        "\n",
        "\n",
        "class ResNN(nn.Module):\n",
        "    def __init__(self, din, m, dout, nTh=2):\n",
        "        \"\"\"\n",
        "            ResNet N portion of Phi\n",
        "        :param d:   int, dimension of space input (expect inputs to be d+1 for space-time)\n",
        "        :param m:   int, hidden dimension\n",
        "        :param nTh: int, number of resNet layers , (number of theta layers)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if nTh < 2:\n",
        "            print(\"nTh must be an integer >= 2\")\n",
        "            exit(1)\n",
        "\n",
        "        self.din = din\n",
        "        self.dout = dout\n",
        "        self.m = m\n",
        "        self.nTh = nTh\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.layers.append(nn.Linear(din, m, bias=True)) # opening layer\n",
        "        self.layers.append(nn.Linear(m,m, bias=True)) # resnet layers\n",
        "        for i in range(nTh-2):\n",
        "            self.layers.append(copy.deepcopy(self.layers[1]))\n",
        "        self.layers.append(nn.Linear(m, dout)) # output layer\n",
        "        self.act = antiderivTanh\n",
        "        self.h = 1.0 / (self.nTh-1) # step size for the ResNet\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            N(s;theta). the forward propogation of the ResNet\n",
        "        :param x: tensor nex-by-d+1, inputs\n",
        "        :return:  tensor nex-by-1,   outputs\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.act(self.layers[0].forward(x))\n",
        "\n",
        "        for i in range(1,self.nTh):\n",
        "            x = x + self.h * self.act(self.layers[i](x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5jibpzNWFjt"
      },
      "outputs": [],
      "source": [
        "class ResNNrk4(nn.Module):\n",
        "    def __init__(self, din, m, dout, nTh=2):\n",
        "        \"\"\"\n",
        "            ResNet N portion of Phi\n",
        "        :param d:   int, dimension of space input (expect inputs to be d+1 for space-time)\n",
        "        :param m:   int, hidden dimension\n",
        "        :param nTh: int, number of resNet layers , (number of theta layers)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if nTh < 2:\n",
        "            print(\"nTh must be an integer >= 2\")\n",
        "            exit(1)\n",
        "\n",
        "        self.din = din\n",
        "        self.dout = dout\n",
        "        self.m = m\n",
        "        self.nTh = nTh\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.layers.append(nn.Linear(din, m, bias=True)) # opening layer\n",
        "        self.layers.append(nn.Linear(m,m, bias=True)) # resnet layers\n",
        "        for i in range(nTh-2):\n",
        "            self.layers.append(copy.deepcopy(self.layers[1]))\n",
        "        self.layers.append(nn.Linear(m, dout)) # output layer\n",
        "        self.act = antiderivTanh\n",
        "        self.h = 1.0 / (self.nTh-1) # step size for the ResNet\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            N(s;theta). the forward propogation of the ResNet\n",
        "        :param x: tensor nex-by-d+1, inputs\n",
        "        :return:  tensor nex-by-1,   outputs\n",
        "        \"\"\"\n",
        "  \n",
        "        x = self.act(self.layers[0].forward(x))\n",
        "\n",
        "        for i in range(1,self.nTh):\n",
        "            k1= self.h / 6 * self.act(self.layers[i].forward(x))\n",
        "            k2= self.h / 3 * self.act(self.layers[i].forward(x+self.h*k1/2))\n",
        "            k3= self.h / 3 * self.act(self.layers[i].forward(x+self.h*k2/2))\n",
        "            k4= self.h / 6 * self.act(self.layers[i].forward(x+self.h*k3))\n",
        "            x = x + self.h * (k1+k2+k3+k4)\n",
        "        x = self.layers[-1](x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWu-K_-nv3Jr"
      },
      "outputs": [],
      "source": [
        "class HINNrk4(nn.Module):\n",
        "    def __init__(self, din, m, dout, nTh=2):\n",
        "        \"\"\"\n",
        "            ResNet N portion of Phi\n",
        "        :param d:   int, dimension of space input (expect inputs to be d+1 for space-time)\n",
        "        :param m:   int, hidden dimension\n",
        "        :param nTh: int, number of resNet layers , (number of theta layers)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if nTh < 2:\n",
        "            print(\"nTh must be an integer >= 2\")\n",
        "            exit(1)\n",
        "\n",
        "        self.din = din\n",
        "        self.dout = dout\n",
        "        self.m = m\n",
        "        self.nTh = nTh\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.layers.append(nn.Linear(din, m, bias=True)) # opening layer\n",
        "        self.layers.append(nn.Linear(m,m, bias=True)) # resnet layers\n",
        "        for i in range(nTh-2): #middle hidden layers\n",
        "            self.layers.append(copy.deepcopy(self.layers[1]))\n",
        "        self.layers.append(nn.Linear(m, dout)) # output layer\n",
        "        self.act = antiderivTanh\n",
        "        self.h = 1.0 / (self.nTh-1) # step size for the ResNet\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            N(s;theta). the forward propogation of the ResNet\n",
        "        :param x: tensor nex-by-d+1, inputs\n",
        "        :return:  tensor nex-by-1,   outputs\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.act(self.layers[0].forward(x))\n",
        "        y = x\n",
        "        z = 0 * y\n",
        "\n",
        "\n",
        "        for i in range(1,self.nTh):\n",
        "          #hamiltonian discritized equations\n",
        "            k1_1= self.h  * self.act(self.layers[i].forward(y))\n",
        "            k2_1= self.h  * self.act(self.layers[i].forward(y+self.h*k1_1/2))\n",
        "            k3_1= self.h  * self.act(self.layers[i].forward(y+self.h*k2_1/2))\n",
        "            k4_1= self.h  * self.act(self.layers[i].forward(y+self.h*k3_1))\n",
        "            y = y + 1/6 * self.h * (k1_1+2*k2_1+2*k3_1+k4_1)\n",
        "\n",
        "            k1_2= self.h  * self.act(self.layers[i].forward(z))\n",
        "            k2_2= self.h  * self.act(self.layers[i].forward(z+self.h*k1_1/2))\n",
        "            k3_2= self.h  * self.act(self.layers[i].forward(z+self.h*k2_1/2))\n",
        "            k4_2= self.h  * self.act(self.layers[i].forward(z+self.h*k3_1))\n",
        "\n",
        "            z = z + 1/6 * self.h * (k1_1+2*k2_1+2*k3_1+k4_1)\n",
        "\n",
        "        y =  self.layers[-1](y)\n",
        "        z =  self.layers[-1](z)\n",
        "\n",
        "        # print(\"y size:\")\n",
        "        # print(y.size())\n",
        "        # print(\"z size:\")\n",
        "        # print(z.size())\n",
        "\n",
        "\n",
        "        x = torch.cat((y,z),dim = 1)\n",
        "        # print(x.size())\n",
        "        # x = self.layers[-1](x) #error occurs here\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry_zatm4eMj8"
      },
      "outputs": [],
      "source": [
        "class HINNVerlet(nn.Module):\n",
        "    def __init__(self, din, m, dout, nTh=2):\n",
        "        \"\"\"\n",
        "            ResNet N portion of Phi\n",
        "        :param d:   int, dimension of space input (expect inputs to be d+1 for space-time)\n",
        "        :param m:   int, hidden dimension\n",
        "        :param nTh: int, number of resNet layers , (number of theta layers)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if nTh < 2:\n",
        "            print(\"nTh must be an integer >= 2\")\n",
        "            exit(1)\n",
        "\n",
        "        self.din = din\n",
        "        self.dout = dout\n",
        "        self.m = m\n",
        "        self.nTh = nTh\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.layers.append(nn.Linear(din, m, bias=True)) # opening layer\n",
        "        self.layers.append(nn.Linear(m,m, bias=True)) # resnet layers\n",
        "        for i in range(nTh-2): #middle hidden layers\n",
        "            self.layers.append(copy.deepcopy(self.layers[1]))\n",
        "        self.layers.append(nn.Linear(m, dout)) # output layer\n",
        "        self.act = antiderivTanh\n",
        "        self.h = 1.0 / (self.nTh-1) # step size for the ResNet\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            N(s;theta). the forward propogation of the ResNet\n",
        "        :param x: tensor nex-by-d+1, inputs\n",
        "        :return:  tensor nex-by-1,   outputs\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.act(self.layers[0].forward(x))\n",
        "        y = x\n",
        "        z = 0 * y\n",
        "\n",
        "\n",
        "        for i in range(1,self.nTh):\n",
        "          #hamiltonian discritized equations\n",
        "          z = z - self.h*self.act(self.layers[i].forward(y)) #transpose?\n",
        "          y = y + self.h*self.act(self.layers[i].forward(z))\n",
        "\n",
        "        y =  self.layers[-1](y)\n",
        "        z =  self.layers[-1](z)\n",
        "        \n",
        "        # print(\"y size:\")\n",
        "        # print(y.size())\n",
        "        # print(\"z size:\")\n",
        "        # print(z.size())\n",
        "\n",
        "\n",
        "        x = torch.cat((y,z),dim = 1)\n",
        "        # print(x.size())\n",
        "        # x = self.layers[-1](x) #error occurs here\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVaOOPZSCOIx"
      },
      "source": [
        "Headers for printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS4uPoGeCNoI"
      },
      "outputs": [],
      "source": [
        "def print_headers( verbose: bool = True):\n",
        "    r\"\"\"\n",
        "    Print headers for nice training\n",
        "    \"\"\"\n",
        "    loss_printouts = ('loss',)\n",
        "    n_loss = len(loss_printouts)\n",
        "\n",
        "    headers = (('', '', '|', 'running',) + (n_loss - 1) * ('',) + ('|', 'train',)\n",
        "               + (n_loss - 1) * ('',) + ('|', 'valid',) + (n_loss - 1) * ('',))\n",
        "\n",
        "    printouts = ('epoch', 'time') + 3 * (('|',) + loss_printouts)\n",
        "    printouts_frmt = '{:<15d}{:<15.4f}' + 3 * ('{:<2s}' + n_loss * '{:<15.4e}')\n",
        "\n",
        "    if verbose:\n",
        "        print(('{:<15s}{:<15s}' + 3 * ('{:<2s}' + n_loss * '{:<15s}')).format(*headers))\n",
        "        print(('{:<15s}{:<15s}' + 3 * ('{:<2s}' + n_loss * '{:<15s}')).format(*printouts))\n",
        "\n",
        "    return headers, printouts, printouts_frmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwBdF-5ACkYl",
        "outputId": "ab82050c-6a04-4bf3-97e4-c432b0469a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              | running        | train          | valid          \n",
            "epoch          time           | loss           | loss           | loss           \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('', '', '|', 'running', '|', 'train', '|', 'valid'),\n",
              " ('epoch', 'time', '|', 'loss', '|', 'loss', '|', 'loss'),\n",
              " '{:<15d}{:<15.4f}{:<2s}{:<15.4e}{:<2s}{:<15.4e}{:<2s}{:<15.4e}')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print_headers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0TC7xMBcwZK"
      },
      "source": [
        "Create a training, validation, and test set for Peaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG4wVlgEddMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acc9999-02a4-4483-93c7-79176242e3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4000, 2])\n"
          ]
        }
      ],
      "source": [
        "n_train = 4000      # number of training points\n",
        "n_val = 300         # number of validation points\n",
        "n_test = 300        # number of testing points\n",
        "#import data and assign each array to a different value\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "odeDataPath = Path('sho_data.npz') #path to file\n",
        "odeData = np.load(odeDataPath)\n",
        "yData = np.reshape(odeData['y'],[-1,1]) #reshape the data so its [datapoints x 1] tensor\n",
        "zData = np.reshape(odeData['z'],[-1,1])\n",
        "tData = np.reshape(odeData['t'],[-1,1])\n",
        "\n",
        "# assign data to x and y\n",
        "x = torch.tensor(tData)\n",
        "y = np.concatenate([yData, zData],axis=1) #combine the y and z data\n",
        "y = torch.tensor(y)\n",
        "print(y.size())\n",
        "# no shuffling\n",
        "x_train, y_train = x, y\n",
        "x_val, y_val = x[3:17], y[3:17]\n",
        "x_test, y_test = x[7:28], y[7:28]\n",
        "# shuffle and split data\n",
        "# idx = torch.randperm(n_train + n_val + n_test)\n",
        "# x_train, y_train = x[idx[:n_train]], y[idx[:n_train]]\n",
        "# x_val, y_val = x[idx[n_train:n_train + n_val]], y[idx[n_train:n_train + n_val]]\n",
        "# x_test, y_test = x[idx[n_train + n_val:]], y[idx[n_train + n_val:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FazWLBj8-bmh"
      },
      "source": [
        "Choose network and architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WldcZSSI-fBw"
      },
      "outputs": [],
      "source": [
        "width = 16\n",
        "depth = 4\n",
        "f = HINNVerlet(1,width,1) #one output for HINN 2 for the others\n",
        "#print(f)\n",
        "# Pytorch optimizer for the network weights\n",
        "optimizer = torch.optim.Adam(f.parameters(), lr=1e-3) #weight decay is for regularization weight_decay=1e-5 add this for regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vYUL_kTDGRq"
      },
      "source": [
        "Define Mean Square Error loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqJl0rdAEq6Y",
        "outputId": "13d4389d-f690-4267-a7d0-e3d9ce8977f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.2282, -1.2061],\n",
            "        [ 2.2274, -1.2061],\n",
            "        [ 2.2266, -1.2060],\n",
            "        ...,\n",
            "        [12.3514, -4.4770],\n",
            "        [12.3544, -4.4781],\n",
            "        [12.3575, -4.4791]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4poVK1FDJ8Q"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y_true: torch.Tensor,y: torch.Tensor):\n",
        "  return (0.5 / y.shape[0]) * torch.norm(y_true - y.view_as(y_true)) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEyLtvmj_B8X"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJl3AcoM_BjQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "44fc19b6-2832-4389-80c9-52ce2ee18cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              | running        | train          | valid          \n",
            "epoch          time           | loss           | loss           | loss           \n",
            "(-1, 0.0, '|', 0, '|', 31.279190690216904, '|', 7.443099038470864)\n",
            "-1             0.0000         | 0.0000e+00     | 3.1279e+01     | 7.4431e+00     \n",
            "0              1.2827         | 2.5391e+00     | 2.0578e+00     | 2.8828e+00     \n",
            "5              1.0215         | 1.7245e+00     | 1.6378e+00     | 2.7674e+00     \n",
            "10             1.0384         | 8.4613e-01     | 7.8201e-01     | 4.7804e-02     \n",
            "15             1.0168         | 6.1495e-01     | 5.6720e-01     | 3.4518e-01     \n",
            "20             1.0095         | 4.9131e-01     | 4.4321e-01     | 6.9312e-02     \n",
            "25             1.0163         | 1.9023e-01     | 1.4853e-01     | 4.8180e-02     \n",
            "30             1.0200         | 3.9399e-02     | 4.2290e-02     | 1.7485e-02     \n",
            "35             1.0437         | 2.0082e-02     | 4.3530e-02     | 3.1354e-02     \n",
            "40             1.0185         | 2.0840e-02     | 6.6845e-03     | 4.7571e-03     \n",
            "45             1.0386         | 1.5524e-02     | 4.4656e-03     | 9.5822e-03     \n",
            "50             1.0218         | 9.3366e-03     | 1.7092e-02     | 1.6854e-02     \n",
            "55             1.0125         | 7.4631e-03     | 8.5481e-03     | 1.5233e-03     \n",
            "60             1.0215         | 1.4534e-02     | 3.2340e-03     | 1.4928e-02     \n",
            "65             1.0406         | 6.0015e-03     | 3.7979e-03     | 8.0341e-03     \n",
            "70             1.0286         | 4.9945e-03     | 6.0107e-03     | 7.1302e-03     \n",
            "75             1.0140         | 7.2485e-03     | 1.2192e-02     | 1.6930e-02     \n",
            "80             1.0310         | 4.9449e-03     | 1.4395e-03     | 8.7670e-04     \n",
            "85             1.0211         | 5.2674e-03     | 1.9204e-03     | 2.7501e-03     \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fcdcb18c88d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# update network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training parameters\n",
        "max_epochs = 100\n",
        "batch_size = 5\n",
        "\n",
        "# get printouts\n",
        "headers, printouts_str, printouts_frmt = print_headers()\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# initial evaluation\n",
        "# mse_loss = nn.MSELoss()\n",
        "\n",
        "\n",
        "loss_train = mse_loss(f(x_train),y_train)\n",
        "loss_val = mse_loss(f(x_val),y_val)\n",
        "\n",
        "n_loss = 1\n",
        "his_iter = (-1, 0.0) + ('|',) + (0,) + ('|',) + (loss_train.item(),) + ('|',) + (loss_val.item(),)\n",
        "print(his_iter)\n",
        "print(printouts_frmt.format(*his_iter))\n",
        "\n",
        "# store history\n",
        "his = np.array([x for x in his_iter if not (x == '|')]).reshape(1, -1)\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# main iteration\n",
        "\n",
        "log_interval = 5 # how often printouts appear\n",
        "for epoch in range(max_epochs):\n",
        "    t0 = time.perf_counter()\n",
        "    # training here\n",
        "    f.train()\n",
        "    n = x_train.shape[0]\n",
        "    b = batch_size\n",
        "    n_batch = n // b\n",
        "    loss = torch.zeros(1)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # shuffle\n",
        "    idx = torch.randperm(n)\n",
        "\n",
        "    for i in range(n_batch):\n",
        "        idxb = idx[i * b:(i + 1) * b]\n",
        "        xb, yb = x_train[idxb], y_train[idxb]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        fb= f(xb)\n",
        "\n",
        "        loss = mse_loss(fb,yb)\n",
        "        running_loss += b * loss.item()\n",
        "\n",
        "        # update network weights\n",
        "        loss.backward(retain_graph= True)\n",
        "        optimizer.step()\n",
        "\n",
        "    running_loss = (running_loss / n,)\n",
        "    \n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # test\n",
        "    loss_train = mse_loss(f,y_train)\n",
        "    loss_val = mse_loss(f(x_val),y_val)\n",
        "    t = t1-t0\n",
        "    his_iter = (epoch, t1 - t0) + ('|',) + running_loss + ('|',) + (loss_train.item(),) + ('|',) + (loss_val.item(),)\n",
        "    if epoch % log_interval == 0:\n",
        "      print(printouts_frmt.format(*his_iter))\n",
        "\n",
        "    # store history\n",
        "    idx = [idx for idx, n in enumerate(np.array([x for x in printouts_str if not (x == '|')])) if n == 'loss'][1]\n",
        "    his = np.concatenate((his, np.array([x for x in his_iter if not (x == '|')]).reshape(1, -1)), axis=0)\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# overall performance on test data\n",
        "loss_test = mse_loss(f(x_test), y_test)\n",
        "print('Test Loss: %0.4e' % loss.item())\n",
        "\n",
        "# convergence plots\n",
        "fig = plt.figure()\n",
        "linewidth = 3\n",
        "idx = [idx for idx, n in enumerate(np.array([x for x in printouts_str if not (x == '|')])) if n == 'loss'][1]\n",
        "\n",
        "plt.semilogy(his[:, 0], his[:, idx], linewidth=linewidth, label='f')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('HINN Verlet Training Loss') #change based on what method you are using\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#fig.savefig('conv_plot.png',dpi=300) #to save the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEVbaUwkUOnk"
      },
      "source": [
        "Testing the network on unseen timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCHumQ_Ss1XL"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "t_test = torch.tensor(np.linspace(0,15,2000)).reshape(-1,1)\n",
        "plt.plot(t_test.detach().numpy(),f(t_test)[:,0].detach().numpy(),x_train.detach().numpy(),f(x_train)[:,1].detach().numpy())\n",
        "plt.title(\"RK4 SHO\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.legend('y','z')\n",
        "plt.show()\n",
        "#fig.savefig('conv_plot2.png',dpi=300) #to save the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETg-3HwbOPNA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#update the inputs to the plt.plot function, but idk how :(\n",
        "plt.plot(sho.y[0], sho.y[1],'b.')\n",
        "plt.xlabel(r\"$y(t)$\", fontsize = 15)\n",
        "plt.ylabel(r\"$\\frac{dy}{dt}$\", fontsize = 15)\n",
        "plt.title('Hamiltonian', fontsize = 15)\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "All Neural Nets",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}